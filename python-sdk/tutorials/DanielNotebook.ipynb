{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d3b439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT with covernet input\n",
    "\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from nuscenes.prediction.models.backbone import ResNetBackbone\n",
    "\n",
    "import utilsHannes as utilsH\n",
    "from utilsHannes import CoverNetNoRelu\n",
    "from utilsHannes import ConstantLatticeLoss\n",
    "from utilsHannes import mean_pointwise_l2_distance\n",
    "importlib.reload(utilsH)\n",
    "\n",
    "#################################################################################################################################\n",
    "# Define your custom dataset class that inherits from torch.utils.data.Dataset\n",
    "class NuscenesDataset(Dataset):\n",
    "    def __init__(self, image_data, agent_state_data, ground_truth_data):\n",
    "        self.image_data = image_data\n",
    "        self.agent_state_data = agent_state_data\n",
    "        self.ground_truth_data = ground_truth_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_data_item = self.image_data[index]\n",
    "        agent_state_data_item = self.agent_state_data[index]\n",
    "        ground_truth_data_item = self.ground_truth_data[index]\n",
    "        \n",
    "        return image_data_item, agent_state_data_item, ground_truth_data_item\n",
    "\n",
    "#################################################################################################################################\n",
    "\n",
    "version = \"v1.0-mini\" # v1.0-mini, v1.0-trainval\n",
    "DATAROOT = \"data/sets/nuscenes\"\n",
    "seconds_of_history_used = 2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfc79091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.533 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nuscenesDev/lib/python3.8/site-packages/nuscenes/map_expansion/map_api.py:1823: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  exteriors = [int_coords(poly.exterior.coords) for poly in polygons]\n",
      "/opt/conda/envs/nuscenesDev/lib/python3.8/site-packages/nuscenes/map_expansion/map_api.py:1824: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  interiors = [int_coords(pi.coords) for poly in polygons for pi in poly.interiors]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.689 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get training data\n",
    "train_subset = \"mini_train\" # 'mini_train', 'mini_val', 'train', 'val'\n",
    "train_img_list, train_img_tensor_list, train_agent_state_vector_list, train_future_xy_local_list = utilsH.get_and_format_data(version, DATAROOT, train_subset, seconds_of_history_used)\n",
    "\n",
    "# Get validation data\n",
    "val_subset = \"mini_val\" # 'mini_train', 'mini_val', 'train', 'val'.\n",
    "val_img_list, val_img_tensor_list, val_agent_state_vector_list, val_future_xy_local_list = utilsH.get_and_format_data(version, DATAROOT, val_subset, seconds_of_history_used)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d6d9684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num_datapoints whole dataset = 49\n",
      "train_num_datapoints short = 49\n",
      "train_img_tensor_list size = torch.Size([3, 500, 500])\n",
      "train_agent_state_vector_list[0] = tensor([2.7443, 1.5398, 0.0000])\n",
      "train_future_xy_local_list[0][0] = [-0.05469174  1.92462069]\n",
      "\n",
      "val_num_datapoints whole dataset = 4\n",
      "val_num_datapoints short = 4\n",
      "val_img_tensor_list size = torch.Size([3, 500, 500])\n",
      "val_agent_state_vector_list[0] = tensor([ 0.1485, -0.0008,  0.0000])\n",
      "val_future_xy_local_list[0][0] = [0.00279729 0.21127512]\n",
      "\n",
      "Training starts:\n",
      "Epoch loss [1/100]: Training: 20.212 | Validation: 4.700\n",
      "Epoch accu [1/100]: Training: 18.4 % | Validation: 0.0 %\n",
      "\n",
      "Epoch loss [2/100]: Training: 16.417 | Validation: 6.327\n",
      "Epoch accu [2/100]: Training: 12.2 % | Validation: 0.0 %\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 104\u001b[0m\n\u001b[1;32m    101\u001b[0m logits \u001b[38;5;241m=\u001b[39m covernet(image_tensor, agent_state_vector)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_trajectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m train_epochLoss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/nuscenes_dolander1/python-sdk/tutorials/utilsHannes.py:453\u001b[0m, in \u001b[0;36mConstantLatticeLoss.__call__\u001b[0;34m(self, batch_logits, batch_ground_truth_trajectory)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m logit, ground_truth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_logits, batch_ground_truth_trajectory):\n\u001b[1;32m    452\u001b[0m     closest_lattice_trajectory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlattice, ground_truth)\n\u001b[0;32m--> 453\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclosest_lattice_trajectory\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(batch_logits\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    454\u001b[0m     classification_loss \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mcross_entropy(logit\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), label)\n\u001b[1;32m    456\u001b[0m     batch_losses \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((batch_losses, classification_loss\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)), \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# For testing\n",
    "train_short_size = 100\n",
    "short_train_img_tensor_list = train_img_tensor_list[:train_short_size]\n",
    "short_train_agent_state_vector_list = train_agent_state_vector_list[:train_short_size]\n",
    "short_train_future_xy_local_list = train_future_xy_local_list[:train_short_size]\n",
    "val_short_size = 25\n",
    "short_val_img_tensor_list = val_img_tensor_list[:val_short_size]\n",
    "short_val_agent_state_vector_list = val_agent_state_vector_list[:val_short_size]\n",
    "short_val_future_xy_local_list = val_future_xy_local_list[:val_short_size]\n",
    "\n",
    "\n",
    "# Prints\n",
    "train_num_datapoints = len(train_img_tensor_list)\n",
    "print(f\"train_num_datapoints whole dataset = {train_num_datapoints}\")\n",
    "train_num_datapoints = len(short_train_img_tensor_list)\n",
    "print(f\"train_num_datapoints short = {train_num_datapoints}\")\n",
    "print(f\"train_img_tensor_list size = {train_img_tensor_list[0].size()}\")\n",
    "print(f\"train_agent_state_vector_list[0] = {train_agent_state_vector_list[0]}\")\n",
    "print(f\"train_future_xy_local_list[0][0] = {train_future_xy_local_list[0][0]}\\n\")\n",
    "val_num_datapoints = len(val_img_tensor_list)\n",
    "print(f\"val_num_datapoints whole dataset = {val_num_datapoints}\")\n",
    "val_num_datapoints = len(short_val_img_tensor_list)\n",
    "print(f\"val_num_datapoints short = {val_num_datapoints}\")\n",
    "print(f\"val_img_tensor_list size = {val_img_tensor_list[0].size()}\")\n",
    "print(f\"val_agent_state_vector_list[0] = {val_agent_state_vector_list[0]}\")\n",
    "print(f\"val_future_xy_local_list[0][0] = {val_future_xy_local_list[0][0]}\\n\")\n",
    "\n",
    "\n",
    "# Variables\n",
    "batch_size = 10\n",
    "shuffle = True # Set to True if you want to shuffle the data in the dataloader\n",
    "num_modes = 64 # 2206, 415, 64 (match with eps_traj_set)\n",
    "eps_traj_set = 8 # 2, 4, 8 (match with num_modes)\n",
    "learning_rate = 1e-4 # From Covernet paper: fixed learning rate of 1e−4\n",
    "num_epochs = 100 \n",
    "\n",
    "# Define datasets\n",
    "train_dataset = NuscenesDataset(train_img_tensor_list, train_agent_state_vector_list, train_future_xy_local_list)\n",
    "train_shortDataset = NuscenesDataset(short_train_img_tensor_list, short_train_agent_state_vector_list, short_train_future_xy_local_list)\n",
    "val_dataset = NuscenesDataset(val_img_tensor_list, val_agent_state_vector_list, val_future_xy_local_list)\n",
    "val_shortDataset = NuscenesDataset(short_val_img_tensor_list, short_val_agent_state_vector_list, short_val_future_xy_local_list)\n",
    "\n",
    "# Instantiate dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "train_shortDataloader = DataLoader(train_shortDataset, batch_size=batch_size, shuffle=shuffle)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "val_shortDataloader = DataLoader(val_shortDataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# Initialize the CoverNet model\n",
    "backbone = ResNetBackbone('resnet50') \n",
    "covernet = CoverNetNoRelu(backbone, num_modes)\n",
    "\n",
    "# Lattice and similarity function\n",
    "with open(f'data/sets/nuscenes-prediction-challenge-trajectory-sets/epsilon_{eps_traj_set}.pkl', 'rb') as f:\n",
    "    lattice = np.array(pickle.load(f))\n",
    "similarity_function = mean_pointwise_l2_distance\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "loss_function = ConstantLatticeLoss(lattice, similarity_function)\n",
    "optimizer = optim.Adam(covernet.parameters(), lr=learning_rate) \n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "covernet.to(device)\n",
    "\n",
    "\n",
    "# Squeeze for correct dimensions\n",
    "for i, train_img_tensor in enumerate(train_img_tensor_list):\n",
    "    train_img_tensor_list[i] = torch.squeeze(train_img_tensor, dim=0)\n",
    "    train_agent_state_vector_list[i] = torch.squeeze(train_agent_state_vector_list[i], dim=0)\n",
    "    \n",
    "for j, val_img_tensor in enumerate(val_img_tensor_list):\n",
    "    val_img_tensor_list[j] = torch.squeeze(val_img_tensor, dim=0)\n",
    "    val_agent_state_vector_list[j] = torch.squeeze(val_agent_state_vector_list[j], dim=0)\n",
    "\n",
    "\n",
    "# Training starts\n",
    "print(\"Training starts:\")\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # TRAINING\n",
    "    train_epochLoss = 0\n",
    "    train_total = 0\n",
    "    train_correct = 0\n",
    "    for train_batchCount, train_batch in enumerate(train_shortDataloader):\n",
    "\n",
    "        # Get train_batch data\n",
    "        image_tensor, agent_state_vector, ground_truth_trajectory = train_batch\n",
    "        \n",
    "        # Send to device\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        agent_state_vector = agent_state_vector.to(device)\n",
    "        ground_truth_trajectory = ground_truth_trajectory.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = covernet(image_tensor, agent_state_vector)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(logits, ground_truth_trajectory)\n",
    "        train_epochLoss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "       # Compute accuracy\n",
    "        for logit, ground_truth in zip(logits, ground_truth_trajectory):\n",
    "            _, predicted = torch.max(logit, 0)\n",
    "            closest_lattice_trajectory = similarity_function(torch.Tensor(lattice).to(device), ground_truth)\n",
    "            train_total += 1\n",
    "            train_correct += (predicted == closest_lattice_trajectory).sum().item()\n",
    "\n",
    "        # Print loss for this train_batch\n",
    "        # print(f\"train_batch [{train_batchCount+1}/{int(train_num_datapoints/batch_size)+1}], Batch Loss: {loss.item():.4f}\")\n",
    "     \n",
    "    \n",
    "    # VALIDATION\n",
    "    val_epochLoss = 0\n",
    "    val_total = 0\n",
    "    val_correct = 0\n",
    "    for val_batchCount, val_batch in enumerate(val_shortDataloader):\n",
    "\n",
    "        # Get val_batch data\n",
    "        image_tensor, agent_state_vector, ground_truth_trajectory = val_batch\n",
    "\n",
    "        # Send to device\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        agent_state_vector = agent_state_vector.to(device)\n",
    "        ground_truth_trajectory = ground_truth_trajectory.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = covernet(image_tensor, agent_state_vector)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(logits, ground_truth_trajectory)\n",
    "        val_epochLoss += loss.item()\n",
    "        \n",
    "        # Compute accuracy\n",
    "        for logit, ground_truth in zip(logits, ground_truth_trajectory):\n",
    "            _, predicted = torch.max(logit, 0)\n",
    "            closest_lattice_trajectory = similarity_function(torch.Tensor(lattice).to(device), ground_truth)\n",
    "            val_total += 1\n",
    "            val_correct += (predicted == closest_lattice_trajectory).sum().item()\n",
    "\n",
    "        # Print loss for this val_batch\n",
    "        # print(f\"val_batch [{val_batchCount+1}/{int(val_num_datapoints/batch_size)+1}], Batch Loss: {loss.item():.4f}\")\n",
    "     \n",
    "    # Print losses for this epoch\n",
    "    print(f\"Epoch loss [{epoch+1}/{num_epochs}]: Training: {train_epochLoss:.3f} | Validation: {val_epochLoss:.3f}\")\n",
    "    print(f\"Epoch accu [{epoch+1}/{num_epochs}]: Training: {train_correct/train_total*100:.1f} % | Validation: {val_correct/val_total*100:.1f} %\\n\")\n",
    "\n",
    "    \n",
    "# Training complete\n",
    "print(\"Training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nuscenesDev",
   "language": "python",
   "name": "nuscenesdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
