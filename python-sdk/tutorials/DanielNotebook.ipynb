{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6d2a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT with covernet input\n",
    "\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from nuscenes.prediction.models.backbone import ResNetBackbone\n",
    "\n",
    "import utilsHannes as utilsH\n",
    "from utilsHannes import CoverNetNoRelu\n",
    "from utilsHannes import ConstantLatticeLoss\n",
    "from utilsHannes import mean_pointwise_l2_distance\n",
    "importlib.reload(utilsH)\n",
    "\n",
    "version = \"v1.0-mini\" # v1.0-mini, v1.0-trainval\n",
    "DATAROOT = \"data/sets/nuscenes\"\n",
    "subset = \"mini_train\" # 'mini_train', 'mini_val', 'train', 'val'.\n",
    "seconds_of_history_used = 2.0\n",
    "\n",
    "#################################################################################################################################\n",
    "# Define your custom dataset class that inherits from torch.utils.data.Dataset\n",
    "class NuscenesDataset(Dataset):\n",
    "    def __init__(self, image_data, agent_state_data, ground_truth_data):\n",
    "        self.image_data = image_data\n",
    "        self.agent_state_data = agent_state_data\n",
    "        self.ground_truth_data = ground_truth_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_data_item = self.image_data[index]\n",
    "        agent_state_data_item = self.agent_state_data[index]\n",
    "        ground_truth_data_item = self.ground_truth_data[index]\n",
    "        \n",
    "        return image_data_item, agent_state_data_item, ground_truth_data_item\n",
    "\n",
    "#################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6757101c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.518 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nuscenesDev/lib/python3.8/site-packages/nuscenes/map_expansion/map_api.py:1823: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  exteriors = [int_coords(poly.exterior.coords) for poly in polygons]\n",
      "/opt/conda/envs/nuscenesDev/lib/python3.8/site-packages/nuscenes/map_expansion/map_api.py:1824: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  interiors = [int_coords(pi.coords) for poly in polygons for pi in poly.interiors]\n"
     ]
    }
   ],
   "source": [
    "# Use get_and_format_data as a wrapping function:\n",
    "img_list, img_tensor_list, agent_state_vector_list, future_xy_local_list = utilsH.get_and_format_data(version, DATAROOT, subset, seconds_of_history_used)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8584541d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_datapoints in whole dataset = 49\n",
      "short_num_datapoints = 49\n",
      "img_tensor_list size = torch.Size([1, 3, 500, 500])\n",
      "agent_state_vector_list = tensor([[2.7443, 1.5398, 0.0000]])\n",
      "future_xy_local_list = [-0.05469174  1.92462069]\n",
      "Epoch [1/100], Epoch Loss: 22.2111\n",
      "Epoch [2/100], Epoch Loss: 15.4978\n",
      "Epoch [3/100], Epoch Loss: 15.2080\n",
      "Epoch [4/100], Epoch Loss: 14.6813\n",
      "Epoch [5/100], Epoch Loss: 13.0910\n",
      "Epoch [6/100], Epoch Loss: 11.9241\n",
      "Epoch [7/100], Epoch Loss: 11.3058\n",
      "Epoch [8/100], Epoch Loss: 11.6602\n",
      "Epoch [9/100], Epoch Loss: 10.0338\n",
      "Epoch [10/100], Epoch Loss: 10.7664\n",
      "Epoch [11/100], Epoch Loss: 9.0260\n",
      "Epoch [12/100], Epoch Loss: 9.6350\n",
      "Epoch [13/100], Epoch Loss: 8.5411\n",
      "Epoch [14/100], Epoch Loss: 7.7895\n",
      "Epoch [15/100], Epoch Loss: 7.2916\n",
      "Epoch [16/100], Epoch Loss: 7.2836\n",
      "Epoch [17/100], Epoch Loss: 6.0003\n",
      "Epoch [18/100], Epoch Loss: 5.1073\n",
      "Epoch [19/100], Epoch Loss: 5.0320\n",
      "Epoch [20/100], Epoch Loss: 4.9066\n",
      "Epoch [21/100], Epoch Loss: 4.2622\n",
      "Epoch [22/100], Epoch Loss: 3.9776\n",
      "Epoch [23/100], Epoch Loss: 3.6681\n",
      "Epoch [24/100], Epoch Loss: 2.9689\n",
      "Epoch [25/100], Epoch Loss: 3.1600\n",
      "Epoch [26/100], Epoch Loss: 2.5112\n",
      "Epoch [27/100], Epoch Loss: 2.1806\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m logits \u001b[38;5;241m=\u001b[39m covernet(image_tensor, agent_state_vector)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_trajectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m epochLoss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/nuscenes_dolander1/python-sdk/tutorials/utilsHannes.py:453\u001b[0m, in \u001b[0;36mConstantLatticeLoss.__call__\u001b[0;34m(self, batch_logits, batch_ground_truth_trajectory)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m logit, ground_truth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_logits, batch_ground_truth_trajectory):\n\u001b[1;32m    452\u001b[0m     closest_lattice_trajectory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlattice, ground_truth)\n\u001b[0;32m--> 453\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclosest_lattice_trajectory\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(batch_logits\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    454\u001b[0m     classification_loss \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mcross_entropy(logit\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), label)\n\u001b[1;32m    456\u001b[0m     batch_losses \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((batch_losses, classification_loss\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)), \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# For testing\n",
    "short_size = 100\n",
    "short_img_tensor_list = img_tensor_list[:short_size]\n",
    "short_agent_state_vector_list = agent_state_vector_list[:short_size]\n",
    "short_future_xy_local_list = future_xy_local_list[:short_size]\n",
    "\n",
    "# Prints\n",
    "num_datapoints = len(img_tensor_list)\n",
    "print(f\"num_datapoints in whole dataset = {num_datapoints}\")\n",
    "num_datapoints = len(short_img_tensor_list)\n",
    "print(f\"short_num_datapoints = {num_datapoints}\")\n",
    "print(f\"img_tensor_list size = {img_tensor_list[0].size()}\")\n",
    "print(f\"agent_state_vector_list = {agent_state_vector_list[0]}\")\n",
    "print(f\"future_xy_local_list = {future_xy_local_list[0][0]}\")\n",
    "\n",
    "\n",
    "# Variables\n",
    "batch_size = 10\n",
    "shuffle = True # Set to True if you want to shuffle the data in the dataloader\n",
    "num_modes = 64 # 2206, 415, 64 (match with eps_traj_set)\n",
    "eps_traj_set = 8 # 2, 4, 8 (match with num_modes)\n",
    "learning_rate = 1e-4 # From Covernet paper: fixed learning rate of 1eâˆ’4\n",
    "num_epochs = 100 \n",
    "\n",
    "# Define your training dataset\n",
    "dataset = NuscenesDataset(img_tensor_list, agent_state_vector_list, future_xy_local_list)\n",
    "shortDataset = NuscenesDataset(short_img_tensor_list, short_agent_state_vector_list, short_future_xy_local_list)\n",
    "\n",
    "# Instantiate your training dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "shortDataloader = DataLoader(shortDataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# Initialize the CoverNet model\n",
    "backbone = ResNetBackbone('resnet50') \n",
    "covernet = CoverNetNoRelu(backbone, num_modes)\n",
    "\n",
    "# Lattice and similarity function\n",
    "with open(f'data/sets/nuscenes-prediction-challenge-trajectory-sets/epsilon_{eps_traj_set}.pkl', 'rb') as f:\n",
    "    lattice = np.array(pickle.load(f))\n",
    "similarity_function = mean_pointwise_l2_distance\n",
    "\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "loss_function = ConstantLatticeLoss(lattice, similarity_function)\n",
    "optimizer = optim.Adam(covernet.parameters(), lr=learning_rate) \n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "covernet.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epochLoss = 0\n",
    "    for batchCount, batch in enumerate(shortDataloader):\n",
    "\n",
    "        # Get batch data\n",
    "        image_tensor, agent_state_vector, ground_truth_trajectory = batch\n",
    "        \n",
    "        # Squeeze for correct dimensions\n",
    "        image_tensor = torch.squeeze(image_tensor, dim=1)\n",
    "        agent_state_vector = torch.squeeze(agent_state_vector, dim=1)\n",
    "\n",
    "        # Send to device\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        agent_state_vector = agent_state_vector.to(device)\n",
    "        ground_truth_trajectory = ground_truth_trajectory.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = covernet(image_tensor, agent_state_vector)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(logits, ground_truth_trajectory)\n",
    "        epochLoss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss for this batch\n",
    "        # print(f\"Batch [{batchCount+1}/{int(num_datapoints/batch_size)+1}], Batch Loss: {loss.item():.4f}\")\n",
    "     \n",
    "    # Print loss for this batch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Epoch Loss: {epochLoss:.4f}\")\n",
    "\n",
    "\n",
    "    # Optionally, you can evaluate the model after each epoch\n",
    "    # by running inference on a validation set and computing relevant metrics\n",
    "\n",
    "# Training complete\n",
    "print(\"Training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nuscenesNewTorch",
   "language": "python",
   "name": "nuscenesnewtorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
