{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utilsHannes as utilsH\n",
    "\n",
    "from nuscenes import NuScenes\n",
    "from nuscenes.prediction import PredictHelper\n",
    "\n",
    "from nuscenes.eval.prediction.splits import get_prediction_challenge_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utilsH)\n",
    "\n",
    "version = \"v1.0-mini\"\n",
    "DATAROOT = \"data/sets/nuscenes\"\n",
    "subset = \"mini_train\"\n",
    "seconds_of_history_used = 2.0\n",
    "mini_train = get_prediction_challenge_split('mini_train', dataroot=DATAROOT)\n",
    "data_set = mini_train\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Use get_and_format_data as a wrapping function:\n",
    "img_list, img_tensor_list, agent_state_vector_list, future_xy_local_list = utilsH.get_and_format_data(version, DATAROOT, subset, seconds_of_history_used)\n",
    "\n",
    "print(img_tensor_list[0])\n",
    "print(agent_state_vector_list[0])\n",
    "print(future_xy_local_list[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(utilsH)\n",
    "\n",
    "# version = \"v1.0-mini\"\n",
    "# DATAROOT = \"data/sets/nuscenes\"\n",
    "# subset = \"mini_train\"\n",
    "\n",
    "# # --------------------------------------------------------------------------\n",
    "# # Use the sub-functions directly:\n",
    "# nuscenes = NuScenes('v1.0-mini', dataroot=DATAROOT)\n",
    "# mini_train = get_prediction_challenge_split('mini_train', dataroot=DATAROOT)\n",
    "# helper = PredictHelper(nuscenes)\n",
    "\n",
    "# # # FOR SMALL TEST:\n",
    "# # data_set = mini_train[:50]\n",
    "# # else:\n",
    "# data_set = mini_train\n",
    "\n",
    "# seconds_of_history_used = 1.5\n",
    "\n",
    "# instance_token_list, sample_token_list = utilsH.get_instance_tokens_and_sample_tokens(data_set)\n",
    "\n",
    "# instance_token_list, sample_token_list = utilsH.remove_short_sequences(seconds_of_history_used, instance_token_list, sample_token_list)\n",
    "\n",
    "# instance_token_list, sample_token_list = utilsH.extract_one_instance_per_sequence(seconds_of_history_used, instance_token_list, sample_token_list)\n",
    "\n",
    "# img_list, agent_state_vector_list, future_xy_local_list = utilsH.get_data_and_ground_truth(nuscenes, helper, seconds_of_history_used, instance_token_list, sample_token_list)\n",
    "\n",
    "# img_tensor_list = utilsH.create_img_tensor(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test objects for specific index\n",
    "\n",
    "test_index = 4 #Den ser ut att bryta mellan 108 och 109 exempelvis\n",
    "\n",
    "print(f\"data_set at index {test_index}: {data_set[test_index]}\")\n",
    "# print(f\"instance_token_list at index {test_index}: {instance_token_list[test_index]}\")\n",
    "# print(f\"sample_token_list at index {test_index}: {sample_token_list[test_index]}\")\n",
    "plt.imshow(img_list[test_index])\n",
    "print(f\"agent_state_vector at index {test_index}: {agent_state_vector_list[test_index]}\")\n",
    "print(f\"future_xy_local at index {test_index}:\")# {future_xy_local_list[test_index]}\")\n",
    "print(future_xy_local_list[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utilsH)\n",
    "\n",
    "## Create video\n",
    "\n",
    "# output_filename = 'test_output_video_2fps_dt2_50.avi'\n",
    "output_filename = f'test_output_video_2fps_length_{len(img_list)}.avi'#Daniel\n",
    "utilsH.create_video(img_list, output_filename, fps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.prediction.models.backbone import ResNetBackbone\n",
    "from nuscenes.prediction.models.mtp import MTP\n",
    "from nuscenes.prediction.models.covernet import CoverNet\n",
    "from nuscenes.prediction.models.covernet import ConstantLatticeLoss\n",
    "\n",
    "import torch\n",
    "\n",
    "backbone = ResNetBackbone('resnet50')\n",
    "mtp = MTP(backbone, num_modes=2)\n",
    "\n",
    "# Note that the value of num_modes depends on the size of the lattice used for CoverNet.\n",
    "covernet = CoverNet(backbone, num_modes=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = 45\n",
    "\n",
    "logits = covernet(img_tensor_list[test_index], agent_state_vector_list[test_index])\n",
    "\n",
    "sofmaxy = torch.nn.Softmax(dim=1)\n",
    "outputy = sofmaxy(logits)\n",
    "print(torch.sum(outputy).item())\n",
    "print(outputy.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Epsilon is the amount of coverage in the set, \n",
    "#i.e. a real world trajectory is at most 8 meters from a trajectory in this set\n",
    "#We released the set for epsilon = 2, 4, 8. Consult the paper for more information\n",
    "#on how this set was created\n",
    "\n",
    "#PATH_TO_EPSILON_8_SET = \"/data/sets/nuscenes-prediction-challenge-trajectory-sets/nuscenes-prediction-challenge-trajectory-sets/epsilon_8.pkl\"\n",
    "PATH_TO_EPSILON_8_SET = \"data/sets/nuscenes-prediction-challenge-trajectory-sets/epsilon_8.pkl\"\n",
    "trajectories = pickle.load(open(PATH_TO_EPSILON_8_SET, 'rb'))\n",
    "\n",
    "#Saved them as a list of lists\n",
    "trajectories = torch.Tensor(trajectories)\n",
    "\n",
    "# Hannes kommenterar bort\n",
    "#Print 5 most likely predictions\n",
    "# print(trajectories[logits.argsort(descending=True)[:1]])\n",
    "\n",
    "\n",
    "\n",
    "# Hannes \n",
    "newTraj = trajectories[logits.argsort(descending=True)[:1]]\n",
    "print(newTraj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hannes cell\n",
    "import numpy as np\n",
    "\n",
    "### Choose which trajectory sets to use \n",
    "# with open('data/sets/nuscenes-prediction-challenge-trajectory-sets/epsilon_2.pkl', 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "with open('data/sets/nuscenes-prediction-challenge-trajectory-sets/epsilon_4.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "# with open('data/sets/nuscenes-prediction-challenge-trajectory-sets/epsilon_8.pkl', 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "\n",
    "allTrajectories = np.array(data)\n",
    "print(f\"Shape of set of all trajectories = {allTrajectories.shape}\")\n",
    "\n",
    "### Plot all trajectories\n",
    "modes = np.arange(allTrajectories.shape[0])\n",
    "timesteps = np.arange(12) # 12 timesteps (which is maximum), can be reduced\n",
    "for mode in modes:\n",
    "    plt.plot(allTrajectories[mode,timesteps,0],allTrajectories[mode,timesteps,1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### Plot selected trajectories\n",
    "numbersOfDisplayedProbabilities = 5\n",
    "# topPredictedTrajectories = allTrajectories[logits.argsort(descending=True)[0][:numbersOfDisplayedProbabilities]] # Choose top 5 predictions\n",
    "topPredictedTrajectories = allTrajectories[outputy.argsort(descending=True)[0][:numbersOfDisplayedProbabilities]] # Choose top 5 predictions\n",
    "\n",
    "# print(allTrajectories)\n",
    "# print(f\"outputy.argsort(descending=True)[0][:numbersOfDisplayedProbabilities] = {outputy.argsort(descending=True)[0][:numbersOfDisplayedProbabilities]}\")\n",
    "# print(topPredictedTrajectories)\n",
    "# predictedProbabilities = outputy[0][outputy.argsort(descending=True)[0][:5]]\n",
    "\n",
    "# tmpOutputy = outputy.detach().clone()\n",
    "# print(tmpOutputy)\n",
    "# predictedProbabilities = []\n",
    "# for topPrediction in range(numbersOfDisplayedProbabilities):\n",
    "#     predictedProbabilitiesIndex = [tmpOutputy.argmax()]\n",
    "#     print(predictedProbabilitiesIndex)\n",
    "#     tmpOutputy[0][predictedProbabilitiesIndex] = 0\n",
    "#     predictedProbabilities.append(predictedProbabilitiesIndex)\n",
    "\n",
    "## TO GET INDICES\n",
    "outputyValues = outputy.detach().numpy().squeeze()\n",
    "predictedProbabilitiesIndices = np.argsort(outputyValues)\n",
    "predictedProbabilities = outputyValues[predictedProbabilitiesIndices]\n",
    "# topProbabilitiesIndices = predictedProbabilities[0][::-1][0:numbersOfDisplayedProbabilities]\n",
    "topProbabilitiesIndices = predictedProbabilitiesIndices[::-1][0:numbersOfDisplayedProbabilities]\n",
    "# predictedProbabilities = outputy.argsort(descending=True)\n",
    "# topProbabilities = predictedProbabilities[0][0:5]\n",
    "# topProbabilities = topProbabilitiesIndices\n",
    "\n",
    "# print(outputyValues)\n",
    "## TO GET VALUES \n",
    "topProbabilities = []\n",
    "for index in topProbabilitiesIndices:\n",
    "    topProbabilities.append(outputyValues[index])\n",
    "\n",
    "\n",
    "# print(f\"outputy = {outputy}\")\n",
    "print(f\"predictedProbabilities = {predictedProbabilities}\")\n",
    "print(f\"predictedProbabilitiesIndices = {predictedProbabilitiesIndices}\")\n",
    "print(f\"topProbabilitiesIndices = {topProbabilitiesIndices}\")\n",
    "print(f\"topProbabilities = {topProbabilities}\")\n",
    "print(f\"Shape of top predicted trajectories = {topPredictedTrajectories.shape}\")\n",
    "modes2 = np.arange(topPredictedTrajectories.shape[0])\n",
    "# timesteps = np.arange(12)\n",
    "for mode in modes2:\n",
    "    plt.plot(allTrajectories[mode,timesteps,0],allTrajectories[mode,timesteps,1])\n",
    "plt.ylim([-10,120])\n",
    "plt.xlim([-40,40])\n",
    "# plt.legend([\"1th prediction\", \"2nd prediction\", \"3rd prediction\", \"4th prediction\", \"5th prediction\"])\n",
    "plt.legend(topProbabilities)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Test line for pushing, again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT with covernet input\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from nuscenes.prediction.models.backbone import ResNetBackbone\n",
    "from nuscenes.prediction.models.mtp import MTP\n",
    "from nuscenes.prediction.models.covernet import CoverNet\n",
    "from nuscenes.prediction.models.covernet import ConstantLatticeLoss\n",
    "from nuscenes.prediction.models.covernet import mean_pointwise_l2_distance\n",
    "\n",
    "importlib.reload(utilsH)\n",
    "\n",
    "version = \"v1.0-mini\" # v1.0-mini, v1.0-trainval\n",
    "DATAROOT = \"data/sets/nuscenes\"\n",
    "subset = \"mini_train\" #One of 'mini_train', 'mini_val', 'train', 'val'.\n",
    "seconds_of_history_used = 2.0\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Use get_and_format_data as a wrapping function:\n",
    "img_list, img_tensor_list, agent_state_vector_list, future_xy_local_list = utilsH.get_and_format_data(version, DATAROOT, subset, seconds_of_history_used)\n",
    "\n",
    "#################################################################################################################################\n",
    "# Define your custom dataset class that inherits from torch.utils.data.Dataset\n",
    "class NuscenesDataset(Dataset):\n",
    "    def __init__(self, image_data, agent_state_data, ground_truth_data):\n",
    "        self.image_data = image_data\n",
    "        self.agent_state_data = agent_state_data\n",
    "        self.ground_truth_data = ground_truth_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_data_item = self.image_data[index]\n",
    "        agent_state_data_item = self.agent_state_data[index]\n",
    "        ground_truth_data_item = self.ground_truth_data[index]\n",
    "        \n",
    "        return image_data_item, agent_state_data_item, ground_truth_data_item\n",
    "\n",
    "#################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints\n",
    "num_datapoints = len(img_tensor_list)\n",
    "print(f\"num_datapoints = {num_datapoints}\")\n",
    "print(f\"img_tensor_list = {img_tensor_list[0].size()}\")\n",
    "print(f\"agent_state_vector_list = {agent_state_vector_list[0]}\")\n",
    "print(f\"future_xy_local_list = {future_xy_local_list[0][0]}\")\n",
    "\n",
    "# For testing\n",
    "short_size = 100\n",
    "short_img_tensor_list = img_tensor_list[:short_size]\n",
    "short_agent_state_vector_list = agent_state_vector_list[:short_size]\n",
    "short_future_xy_local_list = future_xy_local_list[:short_size]\n",
    "\n",
    "# Prints\n",
    "num_datapoints = len(short_img_tensor_list)\n",
    "print(f\"short_num_datapoints = {num_datapoints}\")\n",
    "\n",
    "# Define your dataset\n",
    "dataset = NuscenesDataset(img_tensor_list, agent_state_vector_list, future_xy_local_list)\n",
    "shortDataset = NuscenesDataset(short_img_tensor_list, short_agent_state_vector_list, short_future_xy_local_list)\n",
    "\n",
    "# Instantiate your dataloader\n",
    "batch_size = 10 # 32\n",
    "shuffle = True # Set to True if you want to shuffle the data\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "shortDataloader = DataLoader(shortDataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# Initialize the CoverNet model\n",
    "backbone = ResNetBackbone('resnet50') \n",
    "num_modes = 64 # 2206, 415, 64\n",
    "covernet = CoverNet(backbone, num_modes)\n",
    "\n",
    "# Lattice and similarity function\n",
    "traj_epsilon = 8\n",
    "with open(f'data/sets/nuscenes-prediction-challenge-trajectory-sets/epsilon_{traj_epsilon}.pkl', 'rb') as f:\n",
    "    latticeData = pickle.load(f)\n",
    "lattice = np.array(latticeData) # a numpy array of shape [num_modes, n_timesteps, state_dim]\n",
    "print(f\"latticeSize = {len(lattice[0])}\")\n",
    "similarity_function = mean_pointwise_l2_distance  # You can also define your own similarity function\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "loss_function = ConstantLatticeLoss(lattice, similarity_function)\n",
    "lr = 1e-4 # From Covernet paper: fixed learning rate of 1eâˆ’4\n",
    "optimizer = optim.Adam(covernet.parameters(), lr=lr)  # Replace <YOUR_LEARNING_RATE> with your desired learning rate\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "covernet.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50  # Replace <YOUR_NUM_EPOCHS> with the number of epochs you want to train for\n",
    "for epoch in range(num_epochs):\n",
    "    epochLoss = 0\n",
    "    for batchCount, batch in enumerate(shortDataloader):\n",
    "\n",
    "        # Get batch data\n",
    "        image_tensor, agent_state_vector, ground_truth_trajectory = batch\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        agent_state_vector = agent_state_vector.to(device)\n",
    "        ground_truth_trajectory = ground_truth_trajectory.to(device)\n",
    "\n",
    "        # print(f\"image_tensor before squeeze = {image_tensor.size()}\")\n",
    "        image_tensor = torch.squeeze(image_tensor, dim=1)\n",
    "        # print(f\"image_tensor after squeeze = {image_tensor.size()}\")\n",
    "\n",
    "        # print(f\"agent_state_vector before squeeze = {agent_state_vector.size()}\")\n",
    "        agent_state_vector = torch.squeeze(agent_state_vector, dim=1)\n",
    "        # print(f\"agent_state_vector after squeeze = {agent_state_vector.size()}\")\n",
    "        \n",
    "        # print(f\"ground_truth_trajectory before squeeze = {ground_truth_trajectory.size()}\")\n",
    "        # ground_truth_trajectory = torch.squeeze(ground_truth_trajectory, dim=1)\n",
    "        # print(f\"ground_truth_trajectory after squeeze = {ground_truth_trajectory.size()}\")\n",
    "\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = covernet(image_tensor, agent_state_vector)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(logits, ground_truth_trajectory)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epochLoss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "        # Print loss for this batch\n",
    "        # print(f\"Batch [{batchCount+1}/{int(num_datapoints/batch_size)+1}], Batch Loss: {loss.item():.4f}\")\n",
    "     \n",
    "    # Print loss for this batch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Epoch Loss: {epochLoss:.4f}\")\n",
    "\n",
    "\n",
    "    # Optionally, you can evaluate the model after each epoch\n",
    "    # by running inference on a validation set and computing relevant metrics\n",
    "\n",
    "# Training complete\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test utilsHannes.py functions\n",
    "\n",
    "import importlib\n",
    "import utilsHannes as utilsH\n",
    "importlib.reload(utilsH)\n",
    "\n",
    "try:\n",
    "    utilsH.test_utilsH_functions()\n",
    "    print(\"All tests passed!\")\n",
    "except AssertionError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nuscenesNew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
