{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import pickle\n",
    "import torch\n",
    "# from utilsHannes import mean_pointwise_l2_distance\n",
    "\n",
    "def mean_pointwise_l2_distance(lattice: torch.Tensor, ground_truth: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the index of the closest trajectory in the lattice as measured by l1 distance.\n",
    "    :param lattice: Lattice of pre-generated trajectories. Shape [num_modes, n_timesteps, state_dim]\n",
    "    :param ground_truth: Ground truth trajectory of agent. Shape [1, n_timesteps, state_dim].\n",
    "    :return: Index of closest mode in the lattice.\n",
    "    \"\"\"\n",
    "    stacked_ground_truth = ground_truth.repeat(lattice.shape[0], 1, 1)\n",
    "    return torch.pow(lattice - stacked_ground_truth, 2).sum(dim=2).sqrt().mean(dim=1).argmin()\n",
    "\n",
    "\n",
    "# Load lattice\n",
    "with open('data/sets/nuscenes-prediction-challenge-trajectory-sets/epsilon_8.pkl', 'rb') as f:\n",
    "    trajectories8 = pickle.load(f)\n",
    "lattice8 = np.array(trajectories8)\n",
    "              \n",
    "with open('data/sets/nuscenes-prediction-challenge-trajectory-sets/epsilon_4.pkl', 'rb') as f:\n",
    "    trajectories4 = pickle.load(f)\n",
    "lattice4 = np.array(trajectories4)\n",
    "    \n",
    "with open('data/sets/nuscenes-prediction-challenge-trajectory-sets/epsilon_2.pkl', 'rb') as f:\n",
    "    trajectories2 = pickle.load(f)\n",
    "lattice2 = np.array(trajectories2)\n",
    "\n",
    "dataset_factor_list = [1,1,1,1,1,1,1,1,1,1]\n",
    "lattice_list = [lattice4,lattice4,lattice4,lattice4,lattice4,lattice4,lattice4,lattice4,lattice4,lattice4]\n",
    "modes_list = [415,415,415,415,415,415,415,415,415,415]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Non-IID - Balanced\n",
    "folder_path = \"saveResults/from_other_servers/ClientNumber_non-IID\"\n",
    "num_clients_list = ['1','2','4','8']\n",
    "file_suffix_list = [f\"balanced_C={num_clients}_Cf=1.0_Ef=1.0_B=8_E=1_R=500_Opt=saveFedAvg_Lr=0.0001\" for num_clients in num_clients_list]\n",
    "legend_list = [f'{num_clients}' for num_clients in num_clients_list]\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saveResults/from_other_servers/ClientNumber_non-IID/val_indices_balanced_C=1_Cf=1.0_Ef=1.0_B=8_E=1_R=500_Opt=saveFedAvg_Lr=0.0001.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m train_boolean \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# yes\u001b[39;00m\n\u001b[1;32m      4\u001b[0m num_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(file_suffix_list)\n\u001b[0;32m----> 6\u001b[0m indices_loaded_list \u001b[38;5;241m=\u001b[39m [(np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/val_indices_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))[:num_epochs\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m512\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_factor] \u001b[38;5;28;01mfor\u001b[39;00m file_path, dataset_factor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(file_suffix_list, dataset_factor_list)]\n\u001b[1;32m      7\u001b[0m probabilities_list \u001b[38;5;241m=\u001b[39m [(np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/val_probabilities_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))[:num_epochs\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m512\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_factor] \u001b[38;5;28;01mfor\u001b[39;00m file_path, dataset_factor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(file_suffix_list, dataset_factor_list)]\n\u001b[1;32m      8\u001b[0m ground_truth_loaded_list \u001b[38;5;241m=\u001b[39m [(np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/val_ground_truth_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))[:num_epochs\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m512\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_factor] \u001b[38;5;28;01mfor\u001b[39;00m file_path, dataset_factor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(file_suffix_list, dataset_factor_list)]\n",
      "Cell \u001b[0;32mIn [3], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m train_boolean \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# yes\u001b[39;00m\n\u001b[1;32m      4\u001b[0m num_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(file_suffix_list)\n\u001b[0;32m----> 6\u001b[0m indices_loaded_list \u001b[38;5;241m=\u001b[39m [(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfolder_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/val_indices_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)[:num_epochs\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m512\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_factor] \u001b[38;5;28;01mfor\u001b[39;00m file_path, dataset_factor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(file_suffix_list, dataset_factor_list)]\n\u001b[1;32m      7\u001b[0m probabilities_list \u001b[38;5;241m=\u001b[39m [(np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/val_probabilities_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))[:num_epochs\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m512\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_factor] \u001b[38;5;28;01mfor\u001b[39;00m file_path, dataset_factor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(file_suffix_list, dataset_factor_list)]\n\u001b[1;32m      8\u001b[0m ground_truth_loaded_list \u001b[38;5;241m=\u001b[39m [(np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/val_ground_truth_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))[:num_epochs\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m512\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_factor] \u001b[38;5;28;01mfor\u001b[39;00m file_path, dataset_factor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(file_suffix_list, dataset_factor_list)]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/numpy/lib/npyio.py:407\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    405\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    408\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saveResults/from_other_servers/ClientNumber_non-IID/val_indices_balanced_C=1_Cf=1.0_Ef=1.0_B=8_E=1_R=500_Opt=saveFedAvg_Lr=0.0001.npy'"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "file_number = 0\n",
    "train_boolean = 'no' # yes\n",
    "num_files = len(file_suffix_list)\n",
    "\n",
    "indices_loaded_list = [(np.load(f'{folder_path}/val_indices_{file_path}.npy'))[:num_epochs*512*dataset_factor] for file_path, dataset_factor in zip(file_suffix_list, dataset_factor_list)]\n",
    "probabilities_list = [(np.load(f'{folder_path}/val_probabilities_{file_path}.npy'))[:num_epochs*512*dataset_factor] for file_path, dataset_factor in zip(file_suffix_list, dataset_factor_list)]\n",
    "ground_truth_loaded_list = [(np.load(f'{folder_path}/val_ground_truth_{file_path}.npy'))[:num_epochs*512*dataset_factor] for file_path, dataset_factor in zip(file_suffix_list, dataset_factor_list)]\n",
    "\n",
    "#################################################################################################################################\n",
    "\n",
    "\n",
    "# Print the data\n",
    "print(f\"len(indices_loaded_list) = {len(indices_loaded_list)}\")\n",
    "print(f\"len(probabilities_list) = {len(probabilities_list)}\")\n",
    "print(f\"len(ground_truth_loaded_list) = {len(ground_truth_loaded_list)}\")\n",
    "print(f\"indices_loaded_list[{file_number}].shape = {indices_loaded_list[file_number].shape}\")\n",
    "print(f\"probabilities_list[{file_number}].shape = {probabilities_list[file_number].shape}\")\n",
    "print(f\"ground_truth_loaded_list[{file_number}].shape = {ground_truth_loaded_list[file_number].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot prediction vs ground truth\n",
    "# epoch = 250\n",
    "# index = 16 # 450,83 + 450,32 struggling + 450,33 struggling lots + 450,39 straight + 450,41 + 450,46 straight + 450,63 + 250,4 right + 250,16 left + 250,418 right2\n",
    "epochs = [250, 250, 450, 450]\n",
    "indices = [16, 4, 39, 32]\n",
    "name_list = ['comparePredictedTrajectoriesLeft','comparePredictedTrajectoriesRight','comparePredictedTrajectoriesStraight','comparePredictedTrajectoriesStruggling']\n",
    "\n",
    "\n",
    "for epoch, index, name in zip(epochs, indices, name_list):\n",
    "\n",
    "    if train_boolean == 'yes':\n",
    "        test_index = epoch*2048*dataset_factor_list[file_number] + index\n",
    "    else: \n",
    "        test_index = epoch*512*dataset_factor_list[file_number] + index\n",
    "\n",
    "    nrOfTopTrajectories = 5\n",
    "    predicted_indices = indices_loaded_list[file_number][test_index]\n",
    "    probabilities = probabilities_list[file_number][test_index]\n",
    "    ground_truth_trajectory = ground_truth_loaded_list[file_number][test_index]\n",
    "    closest_lattice_index = mean_pointwise_l2_distance(torch.Tensor(lattice_list[file_number]), torch.Tensor(ground_truth_trajectory))\n",
    "    topProbabilitiesIndices = predicted_indices[:nrOfTopTrajectories]\n",
    "    topProbabilities = probabilities[:nrOfTopTrajectories]\n",
    "\n",
    "    # Lattice ground truth\n",
    "    predicted = topProbabilitiesIndices[0]\n",
    "    print(f\"predicted = {predicted}\")\n",
    "    print(f\"closest_lattice_index = {closest_lattice_index}\")\n",
    "    correct = (predicted == closest_lattice_index)\n",
    "    print(f\"correct = {correct}\")\n",
    "\n",
    "    [plt.plot(lattice_list[file_number][mode,:,0],lattice_list[file_number][mode,:,1], label=f\"Predicted $P$ : {topProbabilities[i]:.2f}\") for i, mode in enumerate(topProbabilitiesIndices)]\n",
    "    plt.plot(ground_truth_trajectory[:,0], ground_truth_trajectory[:,1], label=\"Real Ground truth\")\n",
    "    plt.plot(lattice_list[file_number][closest_lattice_index,:,0],lattice_list[file_number][closest_lattice_index,:,1], label=\"Set $\\kappa_4$ Ground truth\")\n",
    "    plt.ylim([-5,60]); plt.xlim([-30,40]); plt.legend(); \n",
    "    plt.savefig(f\"saveResults/compareDatasets/{name}.pdf\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
