{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218c9a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT with covernet input\n",
    "\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from nuscenes.prediction.models.backbone import ResNetBackbone\n",
    "\n",
    "import utilsHannes as utilsH\n",
    "from utilsHannes import CoverNetNoRelu\n",
    "from utilsHannes import ConstantLatticeLoss\n",
    "from utilsHannes import mean_pointwise_l2_distance\n",
    "importlib.reload(utilsH)\n",
    "\n",
    "#################################################################################################################################\n",
    "# Define your custom dataset class that inherits from torch.utils.data.Dataset\n",
    "class NuscenesDataset(Dataset):\n",
    "    def __init__(self, image_data, agent_state_data, ground_truth_data):\n",
    "        self.image_data = image_data\n",
    "        self.agent_state_data = agent_state_data\n",
    "        self.ground_truth_data = ground_truth_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_data_item = self.image_data[index]\n",
    "        agent_state_data_item = self.agent_state_data[index]\n",
    "        ground_truth_data_item = self.ground_truth_data[index]\n",
    "        \n",
    "        return image_data_item, agent_state_data_item, ground_truth_data_item\n",
    "\n",
    "#################################################################################################################################\n",
    "\n",
    "version = \"v1.0-mini\" # v1.0-mini, v1.0-trainval\n",
    "DATAROOT = \"data/sets/nuscenes\"\n",
    "seconds_of_history_used = 2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8cc4dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.522 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nuscenesDev/lib/python3.8/site-packages/nuscenes/map_expansion/map_api.py:1823: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  exteriors = [int_coords(poly.exterior.coords) for poly in polygons]\n",
      "/opt/conda/envs/nuscenesDev/lib/python3.8/site-packages/nuscenes/map_expansion/map_api.py:1824: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  interiors = [int_coords(pi.coords) for poly in polygons for pi in poly.interiors]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.664 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get training data\n",
    "train_subset = \"mini_train\" # 'mini_train', 'mini_val', 'train', 'val'\n",
    "train_img_list, train_img_tensor_list, train_agent_state_vector_list, train_future_xy_local_list = utilsH.get_and_format_data(version, DATAROOT, train_subset, seconds_of_history_used)\n",
    "\n",
    "# Get validation data\n",
    "val_subset = \"mini_val\" # 'mini_train', 'mini_val', 'train', 'val'.\n",
    "val_img_list, val_img_tensor_list, val_agent_state_vector_list, val_future_xy_local_list = utilsH.get_and_format_data(version, DATAROOT, val_subset, seconds_of_history_used)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b6f3b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num_datapoints whole dataset = 49\n",
      "train_num_datapoints short = 49\n",
      "train_img_tensor_list size = torch.Size([1, 3, 500, 500])\n",
      "train_agent_state_vector_list[0] = tensor([[2.7443, 1.5398, 0.0000]])\n",
      "train_future_xy_local_list[0][0] = [-0.05469174  1.92462069]\n",
      "\n",
      "val_num_datapoints whole dataset = 4\n",
      "val_num_datapoints short = 4\n",
      "val_img_tensor_list size = torch.Size([1, 3, 500, 500])\n",
      "val_agent_state_vector_list[0] = tensor([[ 0.1485, -0.0008,  0.0000]])\n",
      "val_future_xy_local_list[0][0] = [0.00279729 0.21127512]\n",
      "\n",
      "Training starts:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [4, 1, 3, 500, 500]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 139\u001b[0m\n\u001b[1;32m    136\u001b[0m ground_truth_trajectory \u001b[38;5;241m=\u001b[39m ground_truth_trajectory\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mcovernet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_state_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m    142\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(logits, ground_truth_trajectory)\n",
      "File \u001b[0;32m/opt/conda/envs/nuscenesDev/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/nuscenes_dolander1/python-sdk/tutorials/utilsHannes.py:395\u001b[0m, in \u001b[0;36mCoverNetNoRelu.forward\u001b[0;34m(self, image_tensor, agent_state_vector)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_tensor: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    388\u001b[0m                 agent_state_vector: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m        :param image_tensor: Tensor of images in the batch.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m        :param agent_state_vector: Tensor of agent state vectors in the batch\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m        :return: Logits for the batch.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m         backbone_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m         logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([backbone_features, agent_state_vector], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m linear \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead:\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m#             logits = self.relu(linear(logits))\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nuscenesDev/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/nuscenesDev/lib/python3.8/site-packages/nuscenes/prediction/models/backbone.py:60\u001b[0m, in \u001b[0;36mResNetBackbone.forward\u001b[0;34m(self, input_tensor)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    Outputs features after last convolution.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    :param input_tensor:  Shape [batch_size, n_channels, length, width].\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m    :return: Tensor of shape [batch_size, n_convolution_filters]. For resnet50,\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m        the shape is [batch_size, 2048].\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     backbone_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mflatten(backbone_features, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/nuscenesDev/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/nuscenesDev/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/nuscenesDev/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/nuscenesDev/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nuscenesDev/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [4, 1, 3, 500, 500]"
     ]
    }
   ],
   "source": [
    "# For testing\n",
    "train_short_size = 100\n",
    "short_train_img_tensor_list = train_img_tensor_list[:train_short_size]\n",
    "short_train_agent_state_vector_list = train_agent_state_vector_list[:train_short_size]\n",
    "short_train_future_xy_local_list = train_future_xy_local_list[:train_short_size]\n",
    "val_short_size = 25\n",
    "short_val_img_tensor_list = val_img_tensor_list[:val_short_size]\n",
    "short_val_agent_state_vector_list = val_agent_state_vector_list[:val_short_size]\n",
    "short_val_future_xy_local_list = val_future_xy_local_list[:val_short_size]\n",
    "\n",
    "\n",
    "# Prints\n",
    "train_num_datapoints = len(train_img_tensor_list)\n",
    "print(f\"train_num_datapoints whole dataset = {train_num_datapoints}\")\n",
    "train_num_datapoints = len(short_train_img_tensor_list)\n",
    "print(f\"train_num_datapoints short = {train_num_datapoints}\")\n",
    "print(f\"train_img_tensor_list size = {train_img_tensor_list[0].size()}\")\n",
    "print(f\"train_agent_state_vector_list[0] = {train_agent_state_vector_list[0]}\")\n",
    "print(f\"train_future_xy_local_list[0][0] = {train_future_xy_local_list[0][0]}\\n\")\n",
    "val_num_datapoints = len(val_img_tensor_list)\n",
    "print(f\"val_num_datapoints whole dataset = {val_num_datapoints}\")\n",
    "val_num_datapoints = len(short_val_img_tensor_list)\n",
    "print(f\"val_num_datapoints short = {val_num_datapoints}\")\n",
    "print(f\"val_img_tensor_list size = {val_img_tensor_list[0].size()}\")\n",
    "print(f\"val_agent_state_vector_list[0] = {val_agent_state_vector_list[0]}\")\n",
    "print(f\"val_future_xy_local_list[0][0] = {val_future_xy_local_list[0][0]}\\n\")\n",
    "\n",
    "\n",
    "# Variables\n",
    "batch_size = 10\n",
    "shuffle = True # Set to True if you want to shuffle the data in the dataloader\n",
    "num_modes = 64 # 2206, 415, 64 (match with eps_traj_set)\n",
    "eps_traj_set = 8 # 2, 4, 8 (match with num_modes)\n",
    "learning_rate = 1e-4 # From Covernet paper: fixed learning rate of 1eâˆ’4\n",
    "num_epochs = 100 \n",
    "\n",
    "# Define datasets\n",
    "train_dataset = NuscenesDataset(train_img_tensor_list, train_agent_state_vector_list, train_future_xy_local_list)\n",
    "train_shortDataset = NuscenesDataset(short_train_img_tensor_list, short_train_agent_state_vector_list, short_train_future_xy_local_list)\n",
    "val_dataset = NuscenesDataset(val_img_tensor_list, val_agent_state_vector_list, val_future_xy_local_list)\n",
    "val_shortDataset = NuscenesDataset(short_val_img_tensor_list, short_val_agent_state_vector_list, short_val_future_xy_local_list)\n",
    "\n",
    "# Instantiate dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "train_shortDataloader = DataLoader(train_shortDataset, batch_size=batch_size, shuffle=shuffle)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "val_shortDataloader = DataLoader(val_shortDataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# Initialize the CoverNet model\n",
    "backbone = ResNetBackbone('resnet50') \n",
    "covernet = CoverNetNoRelu(backbone, num_modes)\n",
    "\n",
    "# Lattice and similarity function\n",
    "with open(f'data/sets/nuscenes-prediction-challenge-trajectory-sets/epsilon_{eps_traj_set}.pkl', 'rb') as f:\n",
    "    lattice = np.array(pickle.load(f))\n",
    "similarity_function = mean_pointwise_l2_distance\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "loss_function = ConstantLatticeLoss(lattice, similarity_function)\n",
    "optimizer = optim.Adam(covernet.parameters(), lr=learning_rate) \n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "covernet.to(device)\n",
    "\n",
    "\n",
    "# # Squeeze for correct dimensions\n",
    "# for i, train_img_tensor in enumerate(train_img_tensor_list):\n",
    "#     train_img_tensor_list[i] = torch.squeeze(train_img_tensor, dim=0)\n",
    "#     train_agent_state_vector_list[i] = torch.squeeze(train_agent_state_vector_list[i], dim=0)\n",
    "    \n",
    "# for j, val_img_tensor in enumerate(val_img_tensor_list):\n",
    "#     val_img_tensor_list[j] = torch.squeeze(val_img_tensor, dim=0)\n",
    "#     val_agent_state_vector_list[j] = torch.squeeze(val_agent_state_vector_list[j], dim=0)\n",
    "\n",
    "\n",
    "# Training starts\n",
    "print(\"Training starts:\")\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # TRAINING\n",
    "    train_epochLoss = 0\n",
    "    train_total = 0\n",
    "    train_correct = 0\n",
    "    for train_batchCount, train_batch in enumerate(train_shortDataloader):\n",
    "\n",
    "        # Get train_batch data\n",
    "        image_tensor, agent_state_vector, ground_truth_trajectory = train_batch\n",
    "        image_tensor = torch.squeeze(image_tensor, dim=1)\n",
    "        agent_state_vector = torch.squeeze(agent_state_vector, dim=1)\n",
    "        \n",
    "        # Send to device\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        agent_state_vector = agent_state_vector.to(device)\n",
    "        ground_truth_trajectory = ground_truth_trajectory.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = covernet(image_tensor, agent_state_vector)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(logits, ground_truth_trajectory)\n",
    "        train_epochLoss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "       # Compute accuracy\n",
    "        for logit, ground_truth in zip(logits, ground_truth_trajectory):\n",
    "            _, predicted = torch.max(logit, 0)\n",
    "            closest_lattice_trajectory = similarity_function(torch.Tensor(lattice).to(device), ground_truth)\n",
    "            train_total += 1\n",
    "            train_correct += (predicted == closest_lattice_trajectory).sum().item()\n",
    "\n",
    "        # Print loss for this train_batch\n",
    "        # print(f\"train_batch [{train_batchCount+1}/{int(train_num_datapoints/batch_size)+1}], Batch Loss: {loss.item():.4f}\")\n",
    "     \n",
    "    \n",
    "    # VALIDATION\n",
    "    val_epochLoss = 0\n",
    "    val_total = 0\n",
    "    val_correct = 0\n",
    "    for val_batchCount, val_batch in enumerate(val_shortDataloader):\n",
    "\n",
    "        # Get val_batch data\n",
    "        image_tensor, agent_state_vector, ground_truth_trajectory = val_batch\n",
    "        image_tensor = torch.squeeze(image_tensor, dim=1)\n",
    "        agent_state_vector = torch.squeeze(agent_state_vector, dim=1)\n",
    "        \n",
    "\n",
    "        # Send to device\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        agent_state_vector = agent_state_vector.to(device)\n",
    "        ground_truth_trajectory = ground_truth_trajectory.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = covernet(image_tensor, agent_state_vector)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(logits, ground_truth_trajectory)\n",
    "        val_epochLoss += loss.item()\n",
    "        \n",
    "        # Compute accuracy\n",
    "        for logit, ground_truth in zip(logits, ground_truth_trajectory):\n",
    "            _, predicted = torch.max(logit, 0)\n",
    "            closest_lattice_trajectory = similarity_function(torch.Tensor(lattice).to(device), ground_truth)\n",
    "            val_total += 1\n",
    "            val_correct += (predicted == closest_lattice_trajectory).sum().item()\n",
    "\n",
    "        # Print loss for this val_batch\n",
    "        # print(f\"val_batch [{val_batchCount+1}/{int(val_num_datapoints/batch_size)+1}], Batch Loss: {loss.item():.4f}\")\n",
    "     \n",
    "    # Print losses for this epoch\n",
    "    print(f\"Epoch loss [{epoch+1}/{num_epochs}]: Training: {train_epochLoss:.3f} | Validation: {val_epochLoss:.3f}\")\n",
    "    print(f\"Epoch accu [{epoch+1}/{num_epochs}]: Training: {train_correct/train_total*100:.1f} % | Validation: {val_correct/val_total*100:.1f} %\\n\")\n",
    "\n",
    "    \n",
    "# Training complete\n",
    "print(\"Training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nuscenesDev",
   "language": "python",
   "name": "nuscenesdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
